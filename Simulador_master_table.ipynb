{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "cell_execution_strategy": "setup",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMTQTsE9ueGCFyqCy3tGmUl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renatomenendes/AnaliseSobrevivencia/blob/main/Simulador_master_table.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKoKB6YCD3sm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Configuração no pandas\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "# Removendo formatação cientifica\n",
        "pd.options.display.float_format = '{:.1f}'.format\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def score_product_group(row):\n",
        "    if row['product_group'] == 'S10':\n",
        "        return 2\n",
        "    elif row['product_group'] == 'S500':\n",
        "        return 1\n",
        "    elif row['product_group'].startswith('OC_') or row['product_group'] == 'MARITMO':\n",
        "        return 0\n",
        "    elif row['product_group'] == 'OUTROS':\n",
        "        if any(keyword in row['product_name'] for keyword in ['S10', 'S500', 'GRID']):\n",
        "            if 'S10' in row['product_name']:\n",
        "                return 2\n",
        "            elif 'S500' in row['product_name']:\n",
        "                return 1\n",
        "        return 0\n",
        "    return 0\n",
        "\n",
        "def score_product_class(row):\n",
        "    if row['product_class'] == 'Top Premium':\n",
        "        return 3\n",
        "    elif row['product_class'] == 'Premium':\n",
        "        return 2\n",
        "    elif row['product_class'] == 'Convencional':\n",
        "        return 1\n",
        "    return 0"
      ],
      "metadata": {
        "id": "hnl60R1JD_G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_pricing(df, replenishment_cost_col, nominal_discount_col, delta_cost_col, pace_col, minimum_margin_col, standard_neg_value_col):\n",
        "    # Limitar 'pace' entre -2 e 2 e garantir um mínimo para evitar divisão por zero\n",
        "    df[pace_col] = df[pace_col].clip(-2, 2)\n",
        "    pace_abs = np.abs(df[pace_col]).clip(lower=0.1)\n",
        "\n",
        "    # Ajustar 'delta_cost' baseado em 'pace' e 'delta_cost'\n",
        "    df['adjusted_delta_cost'] = np.where(\n",
        "        df[pace_col] > 0,\n",
        "        np.where(\n",
        "            df[delta_cost_col] > 0,\n",
        "            df[delta_cost_col] / pace_abs,  # Suavizar aumento de custo para vendas atrasadas\n",
        "            np.where(\n",
        "                df[delta_cost_col] < 0,\n",
        "                df[delta_cost_col],  # Repassar redução integralmente\n",
        "                0  # Delta_cost zero, sem alteração\n",
        "            )\n",
        "        ),\n",
        "        np.where(\n",
        "            df[pace_col] < 0,\n",
        "            np.where(\n",
        "                df[delta_cost_col] > 0,\n",
        "                df[delta_cost_col] * pace_abs,  # Aumentar custo proporcionalmente para vendas aceleradas\n",
        "                np.where(\n",
        "                    df[delta_cost_col] < 0,\n",
        "                    df[standard_neg_value_col],  # Não repassar reduções, usar valor negativo padrão\n",
        "                    0  # Delta_cost zero, sem alteração\n",
        "                )\n",
        "            ),\n",
        "            np.where(\n",
        "                df[delta_cost_col] > 0,\n",
        "                df[delta_cost_col],  # Aumentar preço proporcionalmente para vendas estáveis\n",
        "                np.where(\n",
        "                    df[delta_cost_col] < 0,\n",
        "                    df[standard_neg_value_col],  # Não repassar reduções, usar valor negativo padrão\n",
        "                    0  # Delta_cost zero, sem alteração\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    potential_price = df[replenishment_cost_col] + df[nominal_discount_col] + df['adjusted_delta_cost']\n",
        "    minimum_price = df[replenishment_cost_col] + df[minimum_margin_col]\n",
        "    df['_initial_price'] = np.maximum(potential_price, minimum_price)\n",
        "    df['_initial_price'] = np.maximum(df['_initial_price'], df[standard_neg_value_col])\n",
        "    return df"
      ],
      "metadata": {
        "id": "SPcVVQaEETeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerando dados\n",
        "start_date = datetime(2024, 1, 1)\n",
        "end_date = datetime.now()\n",
        "date_range = pd.date_range(start_date, end_date, freq='D').to_pydatetime().tolist()\n",
        "n_clients = 5_000\n",
        "n_depots = 80\n",
        "n_products = 20\n",
        "n_samples = 10_000\n",
        "n_uf = 27\n",
        "n_regions = 5\n",
        "n_cities = 4400\n",
        "standard_neg = 10\n",
        "n_components = 10\n",
        "epochs = 100\n",
        "noise_dim = 100\n",
        "target_variable = 'reference_price'\n",
        "\n",
        "client_ids = ['client_' + str(i) for i in range(n_clients)]\n",
        "#depot_ids = load_table(\"processed\",\"depot_register\").query(\"depot in @depots_to_price\")['depot'].tolist()\n",
        "#product_ids = load_table(\"processed\",\"product_register\")['product'].tolist()\n",
        "depot_ids = ['depot_' + str(i) for i in range(n_depots)]\n",
        "product_ids = ['product_' + str(i) for i in range(n_products)]\n",
        "region_ids = ['depot_region_' + str(i) for i in range(n_regions)]\n",
        "state_ids = ['depot_state_' + str(i) for i in range(n_uf)]\n",
        "city_ids = ['depot_city_' + str(i) for i in range(n_cities)]\n",
        "product_group = ['s10', 's500', 'MARÍTIMO', 'OC_A', 'OC_B', 'OUTROS']\n",
        "product_class = ['convencional', 'Premium', 'Top Premium']\n",
        "group_assignments = np.random.choice(product_group, n_products)\n",
        "class_assignments = np.random.choice(product_class, n_products)\n",
        "product_to_group = dict(zip(product_ids, group_assignments))\n",
        "product_to_class = dict(zip(product_ids, class_assignments))\n",
        "\n",
        "granular_cols = ['client','depot','product']"
      ],
      "metadata": {
        "id": "iL9iq1uxEXBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_data(n_samples):\n",
        "    data = {\n",
        "        'date': np.random.choice(date_range, n_samples),\n",
        "        'client': np.random.choice(client_ids, n_samples),\n",
        "        'product': np.random.choice(product_ids, n_samples),\n",
        "        'depot': np.random.choice(depot_ids, n_samples),\n",
        "        'depot_region': np.random.choice(region_ids, n_samples),\n",
        "        'depot_state': np.random.choice(state_ids, n_samples),\n",
        "        'depot_city': np.random.choice(city_ids, n_samples),\n",
        "        'distribution_channel': np.random.choice([10, 11], n_samples),\n",
        "        'forecast': np.random.randint(10, 1000, n_samples),\n",
        "        'volume_m3': np.random.randint(10, 1000, n_samples),\n",
        "        'days_without_purchase': np.random.randint(1, 5, n_samples),\n",
        "        'purchase_frequency': np.ceil(15 / np.random.randint(1, 15, n_samples)),\n",
        "        'pace': np.random.normal(-2.0, 2.0, n_samples),\n",
        "        # 'is_buying': np.random.choice([0, 1], n_samples),\n",
        "        'minimum_margin': np.random.randint(-400, 401, n_samples),\n",
        "        'unitary_margin': np.random.randint(-400, 401, n_samples),\n",
        "        'previous_replenishment_cost': np.random.randint(3500, 7000, n_samples),\n",
        "        'nominal_discount': np.random.randint(-100, 401, n_samples),\n",
        "    }\n",
        "\n",
        "    # Simulando margens e preços coerentes\n",
        "    data['standard_neg'] = standard_neg\n",
        "    data['replenishment_cost'] = data['previous_replenishment_cost'] + np.random.randint(-10, 11, n_samples)\n",
        "    data['reference_price'] = data['replenishment_cost'] + data['unitary_margin'] + standard_neg\n",
        "    data['unitary_margin'] = data['reference_price'] - data['replenishment_cost'] - standard_neg\n",
        "    data['delta_cost'] = (data['replenishment_cost'] - data['previous_replenishment_cost'])\n",
        "    data['pace'] = data['pace'].clip(-2, 2)\n",
        "    data['unitary_cost'] = data['reference_price'] / data['volume_m3']\n",
        "    data['is_buying'] = np.where(data['days_without_purchase'] <= data['purchase_frequency'],1,0)\n",
        "\n",
        "    # Margens e preços comparativos de grupo\n",
        "    price_addition = np.random.randint(100, 1001, n_samples)\n",
        "    data['comparable_group_price'] = data['replenishment_cost'] + price_addition\n",
        "    data['min_corridor_price'] = data['replenishment_cost'] + price_addition\n",
        "    data['max_corridor_price'] = data['min_corridor_price'] + np.random.randint(0, 601, n_samples)\n",
        "\n",
        "    # Margens de corredor seguras\n",
        "    low = data['minimum_margin'] + 1\n",
        "    high = data['minimum_margin'] + np.random.randint(0, 101, n_samples)  # Garante que max_margin >= min_margin\n",
        "    data['min_corridor_margin'] = np.minimum(low, high - 1)  # Garante que min_margin < max_margin\n",
        "    data['max_corridor_margin'] = np.maximum(high, data['min_corridor_margin'] + 1)\n",
        "\n",
        "    # Adicionar variações normais para margens anteriores\n",
        "    adjustment_factor = np.random.normal(loc=0.0, scale=20.0, size=n_samples)\n",
        "    data['previous_margin'] = data['unitary_margin'] + adjustment_factor\n",
        "\n",
        "    # Convertendo para DataFrame\n",
        "    data = pd.DataFrame(data)\n",
        "\n",
        "    # Mapeamento de grupos e classes para produtos\n",
        "    data['product_group'] = data['product'].map(product_to_group)\n",
        "    data['product_class'] = data['product'].map(product_to_class)\n",
        "\n",
        "    # Processamento adicional de datas e dias da semana\n",
        "    weekdays = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
        "    data = data.assign(\n",
        "        year_month=lambda df: df[\"date\"].dt.strftime(\"%Y-%m\"),\n",
        "        nweekday=lambda df: df[\"date\"].dt.weekday,\n",
        "        weekday=lambda df: df[\"nweekday\"].apply(lambda x: weekdays[x])\n",
        "    )\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "LHlP97uMElAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "market_data = simulate_data(n_samples)"
      ],
      "metadata": {
        "id": "rMTxCmDuEoHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = market_data.pipe(calculate_pricing,\n",
        "                      'replenishment_cost',\n",
        "                      'nominal_discount',\n",
        "                      'delta_cost',\n",
        "                      'pace',\n",
        "                      'minimum_margin',\n",
        "                      'standard_neg').assign(diff_price = lambda x:x['_initial_price'] - x['reference_price'],\n",
        "                                             diff_margin = lambda x:(x['_initial_price']-x['replenishment_cost']+x['adjusted_delta_cost'])-(x['reference_price']-x['replenishment_cost']),\n",
        "                                             inferior_limit = lambda x: (x['replenishment_cost']+x['minimum_margin']+x['standard_neg']),\n",
        "                                             )\n",
        "\n",
        "# df.head()[['reference_price',\n",
        "#              'replenishment_cost',\n",
        "#              'delta_cost',\n",
        "#              'adjusted_delta_cost',\n",
        "#              'nominal_discount',\n",
        "#              'minimum_margin',\n",
        "#              'standard_neg',\n",
        "#              'volume_m3',\n",
        "#              'forecast',\n",
        "#              'pace',\n",
        "#              'inferior_limit',\n",
        "#              '_initial_price',\n",
        "#              'diff_price',\n",
        "#              'diff_margin',\n",
        "#             ]]"
      ],
      "metadata": {
        "id": "u6LJQKoMEqnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo características\n",
        "date_features = ['date', 'year_month', 'nweekday', 'weekday']\n",
        "categorical_features = [\"client\", \"product_group\", \"product_class\", \"product\",  \"depot_region\", \"depot_state\",\n",
        "                        \"depot_city\", \"depot\",'distribution_channel','is_buying']\n",
        "numeric_features = ['reference_price','_initial_price','comparable_group_price',\n",
        "                    'previous_replenishment_cost','replenishment_cost',\n",
        "                    'unitary_cost', 'delta_cost', 'adjusted_delta_cost','nominal_discount',\n",
        "                    'max_corridor_margin','minimum_margin','min_corridor_margin',\n",
        "                    'max_corridor_price','inferior_limit','min_corridor_price',\n",
        "                    'forecast', 'volume_m3', 'days_without_purchase', 'purchase_frequency', 'pace',\n",
        "                    'previous_margin','unitary_margin',\n",
        "                    'standard_neg','diff_price', 'diff_margin','_initial_price'\n",
        "                    ]\n",
        "\n",
        "result_columns = ['inferior_limit', 'diff_price', 'diff_margin','_initial_price',]\n",
        "\n",
        "numeric_data_columns = [col for col in numeric_features if col not in result_columns]\n",
        "numeric_data = df[numeric_data_columns]\n",
        "\n",
        "#Organizar o DataFrame\n",
        "columns = [col for col in numeric_data.columns if col != target_variable] + [target_variable]\n",
        "numeric_data = numeric_data[columns]\n",
        "\n",
        "# Seleção de colunas e Preparação dos dados\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df.fillna(df[numeric_features].mean(), inplace=True)\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "P8XL7mSEEto7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizando os dados\n",
        "scaler = RobustScaler()\n",
        "numeric_data_scaled = scaler.fit_transform(numeric_data)\n",
        "\n",
        "# Aplicando PCA\n",
        "pca = PCA(n_components=n_components)\n",
        "principal_components = pca.fit_transform(numeric_data_scaled)\n",
        "\n",
        "# Criando um novo DataFrame com os componentes principais e 'gross_revenue'\n",
        "columns = ['PC{}'.format(1+i) for i in range(n_components)]\n",
        "pca_df = pd.DataFrame(data=principal_components, columns=columns)\n",
        "pca_df[target_variable] = df[target_variable].copy()\n",
        "\n",
        "# Calcular a variação explicada após o ajuste do PCA na primeira abordagem\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "print(\"Variância explicada pelos primeiros componentes principais:\", sum(pca.explained_variance_ratio_))\n",
        "# correlation_with_volume = pca_df.corr()[target_variable].sort_values(ascending=False)\n",
        "# print(correlation_with_volume)\n",
        "\n",
        "# Visualização da variação explicada pelo PCA\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.5, align='center', label='Individual explained variance')\n",
        "plt.step(range(1, len(explained_variance) + 1), np.cumsum(explained_variance), where='mid', label='Cumulative explained variance')\n",
        "plt.ylabel('Ratio of explained variance')\n",
        "plt.xlabel('Principal components')\n",
        "plt.legend(loc='best')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bqrCSeHYIdzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtendo os pesos dos componentes principais\n",
        "components_weights = pca.components_\n",
        "# Criar um DataFrame para visualizar os pesos\n",
        "weights_df = pd.DataFrame(components_weights, columns=numeric_data.columns)\n",
        "# Exibir os pesos do componente principal desejado\n",
        "weights_pc= weights_df.iloc[2].sort_values(ascending=False)\n",
        "significant_fields = weights_pc.drop(target_variable).index.tolist()\n",
        "significant_df = df[significant_fields]\n",
        "display(weights_pc)"
      ],
      "metadata": {
        "id": "xac5STarbp7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline para colunas numéricas com PCA\n",
        "numeric_transformer = Pipeline([\n",
        "                                ('imputer', SimpleImputer(strategy='mean')),\n",
        "                                ('scaler', StandardScaler()),\n",
        "                                ('pca', PCA(n_components=10))  # Usando 10 componentes para uma análise mais detalhada\n",
        "                             ])\n",
        "\n",
        "# Pipeline para colunas categóricas\n",
        "categorical_transformer = Pipeline([\n",
        "                                    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "                                    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "                                  ])\n",
        "\n",
        "# Combinar transformadores\n",
        "preprocessor = ColumnTransformer([\n",
        "                                  ('num', numeric_transformer, numeric_data_columns),\n",
        "                                  ('cat', categorical_transformer, categorical_features)\n",
        "                                ])\n",
        "\n",
        "# Pré-processamento e divisão dos dados\n",
        "X = preprocessor.fit_transform(df[numeric_data_columns + categorical_features])\n",
        "y = df[target_variable]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Acessar o componente PCA dentro do pipeline e obter a variação explicada\n",
        "explained_variance = preprocessor.named_transformers_['num'].named_steps['pca'].explained_variance_ratio_\n",
        "\n",
        "print(\"Variância explicada pelos primeiros componentes principais:\", sum(explained_variance))\n",
        "# correlation_with_volume = pca_df.corr()[target_variable].sort_values(ascending=False)\n",
        "# print(correlation_with_volume)\n",
        "\n",
        "# Visualização da variação explicada pelo PCA\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(range(1, len(explained_variance) + 1),\n",
        "        explained_variance,\n",
        "        alpha=0.5,\n",
        "        align='center',\n",
        "        label='Individual explained variance')\n",
        "plt.step(range(1, len(explained_variance) + 1),\n",
        "         np.cumsum(explained_variance),\n",
        "         where='mid',\n",
        "         label='Cumulative explained variance')\n",
        "plt.ylabel('Ratio of explained variance')\n",
        "plt.xlabel('Principal components')\n",
        "plt.legend(loc='best')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TPMjUsP8MWQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Acessar o componente PCA dentro do pipeline\n",
        "pca_component = preprocessor.named_transformers_['num'].named_steps['pca']\n",
        "# Obter os pesos dos componentes principais\n",
        "components_weights = pca_component.components_\n",
        "# Criar um DataFrame para visualizar os pesos\n",
        "weights_df = pd.DataFrame(components_weights, columns=numeric_data_columns)\n",
        "# Exibir os pesos do componente principal desejado\n",
        "weights_pc = weights_df.iloc[2].sort_values(ascending=False)\n",
        "# Remover o campo target_variable da lista de colunas significativas\n",
        "significant_fields = weights_pc.drop(target_variable).index.tolist()\n",
        "\n",
        "# Criar o DataFrame significant_df usando apenas as colunas relevantes\n",
        "significant_fields = weights_pc.index.tolist()\n",
        "significant_df = df[significant_fields]\n",
        "\n",
        "print(weights_pc)"
      ],
      "metadata": {
        "id": "KtLbkCRq1qux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam, AdamW\n",
        "from torch.nn import BCELoss\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "metadata": {
        "id": "5ghJSPCvVV1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição do dataset\n",
        "class PricingDataset(Dataset):\n",
        "    def __init__(self, dataframe, target_variable):\n",
        "        self.features = dataframe.drop(columns=target_variable).values.astype(np.float32)\n",
        "        self.targets = dataframe[target_variable].values.astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return torch.tensor(self.features[idx], dtype=torch.float), torch.tensor(self.targets[idx], dtype=torch.float)\n"
      ],
      "metadata": {
        "id": "y_lvTj5oVYxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Gerador\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(noise_dim, 256),\n",
        "            nn.BatchNorm1d(256),  # Normalização do Batch\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),  # Normalização do Batch\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),  # Normalização do Batch\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1024, output_dim),\n",
        "            nn.ReLU()  # Função de Ativação ReLU\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "A-x5VyX26-Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Discriminador\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "KYMZQURe7dwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento da GAN\n",
        "def train_gan(data_loader,\n",
        "              generator,\n",
        "              discriminator,\n",
        "              g_optimizer,\n",
        "              d_optimizer,\n",
        "              criterion,\n",
        "              epochs,\n",
        "              noise_dim,\n",
        "              device,):\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for real_features, _ in data_loader:\n",
        "            real_features = real_features.to(device)\n",
        "            batch_size = real_features.size(0)\n",
        "            real_labels = torch.ones(batch_size, 1).to(device)\n",
        "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "            # Treinando o Discriminador\n",
        "            d_optimizer.zero_grad()\n",
        "            real_predictions = discriminator(real_features)\n",
        "            d_loss_real = criterion(real_predictions, real_labels)\n",
        "\n",
        "            noise = torch.randn(batch_size, noise_dim, device=device)\n",
        "            fake_features = generator(noise)\n",
        "            fake_predictions = discriminator(fake_features.detach())\n",
        "            d_loss_fake = criterion(fake_predictions, fake_labels)\n",
        "\n",
        "            d_loss = d_loss_real + d_loss_fake\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "            # Atualizar a taxa de aprendizado do discriminador\n",
        "            d_scheduler.step()\n",
        "\n",
        "            # Treinando o Gerador\n",
        "            g_optimizer.zero_grad()\n",
        "            fake_predictions = discriminator(fake_features)\n",
        "            g_loss = criterion(fake_predictions, real_labels)\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, G Loss: {g_loss.item():.4f}, D Loss: {d_loss.item():.4f}')\n"
      ],
      "metadata": {
        "id": "wdNlhfd1Vr5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurações\n",
        "input_dim = significant_df.drop(columns=[target_variable]).shape[1]\n",
        "\n",
        "# Definindo as dimensões corretas\n",
        "# generator = Generator(noise_dim=100, output_dim=input_dim)\n",
        "# discriminator = Discriminator(input_dim=input_dim)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataset = PricingDataset(dataframe=significant_df, target_variable=target_variable)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "output_dim = significant_df.drop(columns=[target_variable]).shape[1]\n",
        "noise_dim = significant_df.drop(columns=[target_variable]).shape[1]\n",
        "\n",
        "# Configuração do Gerador e do Discriminador\n",
        "gen = Generator(input_dim=input_dim, output_dim=output_dim).to(device)\n",
        "disc = Discriminator(input_dim=output_dim).to(device)\n",
        "\n",
        "# Otimizadores\n",
        "# g_optimizer = Adam(gen.parameters(), lr=0.0001, weight_decay=0.001)\n",
        "# d_optimizer = Adam(disc.parameters(), lr=0.0001)  # Taxa de aprendizado padrão para o discriminador\n",
        "\n",
        "# Ajuste Fino dos Hiperparâmetros\n",
        "learning_rate_g = 0.0002  # Taxa de aprendizado do gerador reduzida\n",
        "learning_rate_d = 0.0002  # Taxa de aprendizado do discriminador reduzida\n",
        "\n",
        "g_optimizer = Adam(gen.parameters(), lr=learning_rate_g)\n",
        "d_optimizer = Adam(disc.parameters(), lr=learning_rate_d)\n",
        "\n",
        "# Regularização Adicional\n",
        "weight_decay = 0.001  # Coeficiente de regularização L2\n",
        "\n",
        "g_optimizer = AdamW(gen.parameters(), lr=learning_rate_g, weight_decay=weight_decay)\n",
        "d_optimizer = AdamW(disc.parameters(), lr=learning_rate_d, weight_decay=weight_decay)\n",
        "\n",
        "# Programação de taxa de aprendizado para o discriminador\n",
        "d_scheduler = torch.optim.lr_scheduler.ExponentialLR(d_optimizer, gamma=0.95)  # Reduz a taxa de aprendizado do discriminador ao longo do tempo\n",
        "\n",
        "criterion = BCELoss()\n",
        "#criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "l-Q8gdwPVyce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(generator,\n",
        "                   discriminator,\n",
        "                   data_loader,\n",
        "                   criterion):\n",
        "    generator.eval()\n",
        "    discriminator.eval()\n",
        "    total_d_loss = 0\n",
        "    total_g_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for features, _ in data_loader:\n",
        "            batch_size = features.size(0)\n",
        "            real_labels = torch.ones(batch_size, 1, device=device)\n",
        "            fake_labels = torch.zeros(batch_size, 1, device=device)\n",
        "\n",
        "            real_predictions = discriminator(features)\n",
        "            d_loss_real = criterion(real_predictions, real_labels)\n",
        "            noise = torch.randn(batch_size, input_dim, device=device)\n",
        "            fake_features = generator(noise)\n",
        "            fake_predictions = discriminator(fake_features)\n",
        "            d_loss_fake = criterion(fake_predictions, fake_labels)\n",
        "            d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "            g_loss = criterion(discriminator(fake_features), real_labels)\n",
        "\n",
        "            total_d_loss += d_loss.item()\n",
        "            total_g_loss += g_loss.item()\n",
        "\n",
        "    avg_d_loss = total_d_loss / len(data_loader)\n",
        "    avg_g_loss = total_g_loss / len(data_loader)\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    return avg_d_loss, avg_g_loss"
      ],
      "metadata": {
        "id": "xPNyFztnlaqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento\n",
        "train_gan(data_loader,\n",
        "          gen,\n",
        "          disc,\n",
        "          g_optimizer,\n",
        "          d_optimizer,\n",
        "          criterion,\n",
        "          epochs=epochs,\n",
        "          noise_dim=noise_dim,\n",
        "          device=device\n",
        "        )"
      ],
      "metadata": {
        "id": "nGxKFj2TWMml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificação das dimensões após a preparação do dataset\n",
        "print(\"Dimensão de entrada para o Discriminador:\", significant_df.drop(columns=[target_variable]).shape[1])\n",
        "print(\"Dimensão de saída do Gerador:\", significant_df.shape[1] - 1)"
      ],
      "metadata": {
        "id": "K3JWv2mBsRB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chamando a função de validação após o treinamento\n",
        "avg_d_loss, avg_g_loss = validate_model(gen, disc, data_loader, criterion)\n",
        "print(\"Perda média do Discriminador na Validação:\", avg_d_loss)\n",
        "print(\"Perda média do Gerador na Validação:\", avg_g_loss)"
      ],
      "metadata": {
        "id": "-_eTU_B2hsKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_prices(real_data, generated_data):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Ajustando a dimensão dos dados, se necessário\n",
        "    if real_data.ndim > 1:\n",
        "        real_data = real_data.flatten()\n",
        "    if generated_data.ndim > 1:\n",
        "        generated_data = generated_data.flatten()\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(real_data, bins=50, density=True, alpha=0.6, color='g')\n",
        "    plt.title('Distribuição de Preços Reais')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(generated_data, bins=50, density=True, alpha=0.6, color='r')\n",
        "    plt.title('Distribuição de Preços Gerados')\n",
        "    plt.show()\n",
        "\n",
        "def validate_and_plot(df, generator, device, noise_dim, num_samples=1000):\n",
        "    # Extraindo amostras dos dados reais\n",
        "    real_data = df.sample(n=num_samples).values\n",
        "    real_data_tensor = torch.tensor(real_data, dtype=torch.float).to(device)\n",
        "\n",
        "    # Gerando dados falsos\n",
        "    noise = torch.randn(num_samples, noise_dim, device=device)\n",
        "    generated_data = generator(noise)\n",
        "\n",
        "    # Convertendo tensores para numpy e desacoplando do grafo de cálculo\n",
        "    real_data_numpy = real_data_tensor.detach().cpu().numpy()\n",
        "    generated_data_numpy = generated_data.detach().cpu().numpy()\n",
        "\n",
        "    # Plotando os resultados\n",
        "    plot_prices(real_data_numpy, generated_data_numpy)\n",
        "\n",
        "# Usando a função\n",
        "validate_and_plot(significant_df, gen, device, noise_dim)"
      ],
      "metadata": {
        "id": "l1QhhLuQ3MET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(epoch, model, optimizer, path='model_checkpoint'):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, os.path.join(path, f'model_epoch_{epoch}.pth'))"
      ],
      "metadata": {
        "id": "HJFJ-XqvGNm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_prices(real_data, generated_data):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(real_data.detach().numpy(), bins=50, density=True, alpha=0.6, color='g')\n",
        "    plt.title('Distribuição de Preços Reais')\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(generated_data.detach().numpy(), bins=50, density=True, alpha=0.6, color='r')\n",
        "    plt.title('Distribuição de Preços Gerados')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "A3t2Z55tGRu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_generated_prices(df, generator, device):\n",
        "    generator.eval()\n",
        "    all_generated_prices = []\n",
        "\n",
        "    # Converter DataFrame para tensor\n",
        "    data_tensor = torch.tensor(df.values.astype(np.float32), device=device)\n",
        "    data_loader = DataLoader(data_tensor, batch_size=32, shuffle=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_features in data_loader:\n",
        "            generated_prices = generator(batch_features).detach().cpu().numpy()\n",
        "            all_generated_prices.extend(generated_prices.flatten())  # Flatten para garantir que seja 1D\n",
        "\n",
        "    # Adicionando os preços gerados ao DataFrame\n",
        "    df['generated_price'] = all_generated_prices\n",
        "    return df\n",
        "\n",
        "# Contagem correta das características de entrada\n",
        "input_features = significant_df.drop(columns=[target_variable], errors='ignore')  # Removendo a coluna alvo para garantir\n",
        "\n",
        "# Verificando a dimensão de entrada\n",
        "input_dim = input_features.shape[1]\n",
        "print(\"Dimensão de entrada confirmada:\", input_dim)\n",
        "\n",
        "# Reinstancia o Gerador com a dimensão de entrada correta\n",
        "generator = Generator(input_dim=input_dim, output_dim=1).to(device)  # Ajuste output_dim conforme necessário\n",
        "\n",
        "# Removendo a coluna 'generated_price' que não deve ser incluída como característica de entrada\n",
        "if 'generated_price' in input_features.columns:\n",
        "    input_features_adjusted = input_features.drop(columns=['generated_price'])\n",
        "else:\n",
        "    input_features_adjusted = input_features\n",
        "\n",
        "# Verifique novamente a dimensão após remover a coluna\n",
        "print(\"Dimensões corrigidas:\", input_features_adjusted.shape)\n",
        "\n",
        "# Aplicar a função para adicionar preços gerados usando o gerador com as dimensões corrigidas\n",
        "df_with_prices = add_generated_prices(input_features_adjusted, generator, device)\n",
        "display(df_with_prices.head().T)\n"
      ],
      "metadata": {
        "id": "5jDCsJJEVoHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adicionar coluna com a discrepância dos preços\n",
        "df_with_prices['price_discrepancy'] = df_with_prices['generated_price'] - (df_with_prices['replenishment_cost'] + df_with_prices['minimum_margin'])\n",
        "\n",
        "# Ordenar pelo valor absoluto da discrepância para identificar os maiores erros\n",
        "df_with_prices['abs_price_discrepancy'] = df_with_prices['price_discrepancy'].abs()\n",
        "worst_cases = df_with_prices.sort_values('abs_price_discrepancy', ascending=False)\n",
        "\n",
        "# Analisar os casos com maior discrepância\n",
        "display(worst_cases[['replenishment_cost', 'minimum_margin', 'generated_price', 'price_discrepancy', 'abs_price_discrepancy']].head())\n"
      ],
      "metadata": {
        "id": "JKOp6fikpH2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ddHnMjzKqOM7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}